{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Annotation Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_path = \"/home/randy/Documents/HuaweiICT/repo/ICT-Huawei/action_video/my_dataset/ann/data1_12.mp4.json\"\n",
    "video_path = \"/home/randy/Documents/HuaweiICT/repo/ICT-Huawei/action_video/my_dataset/video/data1_12.mp4\"\n",
    "fps = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_action_map = {\n",
    "    \"person_1\" : [],\n",
    "    \"person_2\" : [],\n",
    "    \"person_3\" : [],\n",
    "    \"person_4\" : [],\n",
    "    \"person_5\" : [],\n",
    "    \"person_6\" : [],\n",
    "    \"person_7\" : [],\n",
    "    \"person_8\" : [],\n",
    "}\n",
    "\n",
    "for i in range(126):\n",
    "    # Person 1\n",
    "    action_person_1 = [\"walking\"]\n",
    "    if i >= 7:\n",
    "        action_person_1.append(\"staring\")\n",
    "    class_action_map[\"person_1\"].append(action_person_1)\n",
    "\n",
    "    # Person 2\n",
    "    action_person_2 = []\n",
    "    if i <= 45 or i >= 55:\n",
    "        action_person_2.extend([\"standing\",\"staring\"])\n",
    "    class_action_map[\"person_2\"].append(action_person_2)\n",
    "    \n",
    "    # Person 3\n",
    "    action_person_3 = [\"crouch\"]\n",
    "    class_action_map[\"person_3\"].append(action_person_3)\n",
    "\n",
    "    # Person 4\n",
    "    action_person_4 = []\n",
    "    if i <= 16:\n",
    "        action_person_4.extend([\"standing\",\"staring\",\"talking\"])\n",
    "    if i <= 25:\n",
    "        action_person_4.extend([\"walking\"])\n",
    "    if i >= 29 and i <= 37:\n",
    "        action_person_4.extend([\"grabbing\",\"positioning\"])\n",
    "    if i >= 30:\n",
    "        action_person_4.extend([\"pushing\"])\n",
    "    class_action_map[\"person_4\"].append(action_person_4)\n",
    "    \n",
    "    # Person 5\n",
    "    action_person_5 = []\n",
    "    if i <= 26:\n",
    "        action_person_5.extend([\"standing\",\"talking\"])\n",
    "    if i >= 29 and i <= 37:\n",
    "        action_person_5.extend([\"grabbing\",\"positioning\"])\n",
    "    if i >= 30:\n",
    "        action_person_5.extend([\"pulling\"])\n",
    "    class_action_map[\"person_5\"].append(action_person_5)\n",
    "\n",
    "    # Person 6\n",
    "    action_person_6 = []\n",
    "    if i <= 20:\n",
    "        action_person_6.extend([\"slight bowing\",\"staring\"])\n",
    "    if i >= 21:\n",
    "        action_person_6.extend([\"crouch\",\"staring\"])\n",
    "    class_action_map[\"person_6\"].append(action_person_6)\n",
    "\n",
    "    # Person 7\n",
    "    action_person_7 = [\"walking\"]\n",
    "    class_action_map[\"person_7\"].append(action_person_7)\n",
    "\n",
    "    # Person 8\n",
    "    action_person_8 = [\"walking\"]\n",
    "    class_action_map[\"person_8\"].append(action_person_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1_12_annotated.mp4 is Done!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Read the annotation file\n",
    "with open(annot_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Read the meta file\n",
    "meta_path = \"./meta.json\"\n",
    "with open(meta_path,'r') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "objects = annotations[\"objects\"]\n",
    "\n",
    "color_maps = {el[\"title\"] : el[\"color\"] for el in meta[\"classes\"] if el[\"title\"].startswith(\"person\")}\n",
    "\n",
    "object_action_map = {}\n",
    "object_cmap = {}\n",
    "for obj in objects:\n",
    "    key = obj[\"key\"]\n",
    "    class_title = obj[\"classTitle\"]\n",
    "    try:\n",
    "        hex_color = color_maps[class_title]\n",
    "        rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (1, 3, 5))\n",
    "        object_cmap[key] = rgb_color\n",
    "        object_action_map[key] = class_action_map[class_title]\n",
    "    except:\n",
    "        rgb_color = tuple(np.random.randint(0,255) for i in range(3))\n",
    "        object_cmap[key] = rgb_color\n",
    "\n",
    "result_video_path = os.path.split(video_path)[-1].replace(\".mp4\",\"_annotated.mp4\")\n",
    "video_writer = cv2.VideoWriter(result_video_path,\n",
    "                            cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                            fps, (width, height))\n",
    "\n",
    "num_frames_video = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Loop over each frame in the video\n",
    "for n in range(num_frames_video):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Get the annotations for the current frame\n",
    "        frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        frame_annotations = annotations[\"frames\"][frame_number]\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    # Draw bounding boxes around each object in the frame\n",
    "    for fig in frame_annotations[\"figures\"]:\n",
    "        [x1, y1], [x2, y2] = fig['geometry']['points']['exterior']\n",
    "        obj_key = fig[\"objectKey\"]\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), object_cmap[obj_key], 2)\n",
    "\n",
    "        object_key = fig[\"objectKey\"]\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.3\n",
    "        thickness = 1\n",
    "        action = ','.join(object_action_map[object_key][frame_number])\n",
    "        text_size = cv2.getTextSize(action, font, font_scale, thickness)[0]\n",
    "        cv2.putText(frame, action, (x1, y1 - text_size[1]), font, font_scale, (0, 255, 0), thickness)\n",
    "    \n",
    "    # Write the frame to the output video\n",
    "    video_writer.write(frame)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    # cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(60) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_writer.release()\n",
    "# # Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"{result_video_path} is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
